\section{DISCUSSION}
\label{sec:discussion}
In this section, we discuss some challenges that 
were not pointed out in preceding sections (not dealt 
in existing systems) and other opportunities that makes these 
systems more feasible.
We also particularly look at challenges and opportunities 
presented by Aguilera et al.~\cite{Aguilera2017} four 
years ago and highlight some of them that are yet to be 
explored.


\vspace{3pt}
\noindent \uline{Transparency vs. App Integration}
Transparency is important for adoption so, as we've seen, 
most previous systems opt for it. However, as 
language-based runtimes have shown, exploiting 
application hints/semantics for optimizing runtime can be
a key factor in cutting down the performance impact.
Perhaps there is a middle-ground to be found here 
where a system provides both options and the choice is left
to the application to either run without changes or 
provide hints for better performance. For example, 
a kernel-based virtual memory interface that also exposes 
a custom API through 
ioctl (like LITE~\cite{literdma}) to, for example, supply 
application-specific prefetching or call an RPC.


\vspace{3pt}
\noindent \uline{Disaggregated Memory for VMs and beyond.}
Most modern workloads run on some virtualization platform 
or the other (VMs, Containers, Lambdas, etc.)~\cite{Aguilera2017}, 
so there is a need to extend disaggregated memory to such 
platforms for wider adoption. The hypervisor can take the place 
of the kernel in implementing runtime and managing/providing 
the diaggregated memory. However, VMs, unlike processes, come 
with strict SLOs and with an expectation of performance isolation,
so the runtime needs to account for this while managing memory 
(e.g., proper memory accouting, maintaining separate caches, etc.). 
Also, the gap between the application and the runtime is further 
widened making it harder to get app-specific information.


\vspace{3pt}
\noindent \uline{Job scheduling on disaggregated systems.}
Ideally, job scheduling based on its CPU and memory requirements 
should be simple on a disaggregated system because as long 
as the cluster has enough memory for a job, it can be placed  
anywhere. However, non-uniform memory access between local 
and remote memory means that cluster throughput would be 
higher if total number of remote accesses, which can depend
on job placement, is minimized. For example, given similar 
compute requirements, jobs with large working memory are   
better paired with the ones with smaller working sets on the 
individual compute nodes to balance out contention for the 
local DRAM (cache). Similarly, different jobs react differently 
to reduced local memory (cache) size which is another aspect on 
which jobs can be balanced to prioritize the sensitive ones.
This line of work can build on farm-memory aware schedulers 
like Fastswap~\cite{fastswap}.


\vspace{3pt}
\noindent \uline{Memory sharing and consistency model.}
Systems so far have either completely avoided any explicit 
memory sharing across servers or provided only coarse-grained 
sharing with no consistency guarantees. 
If sharing is to be supported for transparent virtual 
memory interface, it should provide same semantics as current 
cache coherence protocols on the entire address space but 
providing such strong consistency model across the network 
is still infeasible (even the recent RDMA-based DSMs~\cite{farm,gam} 
do not attempt it); none of the major (transparent) 
disaggregation systems~\cite{infiniswap,legoos} provided this 
support, perhaps for this reason. Non-transparent ones like 
Remote Regions~\cite{remregions} and LITE~\cite{literdma} provide
support for mapping the same region in multiple servers along with
basic synchronization support like barriers and mutexes for 
synchronizing their accesses, but no consistency on reads/writes. 
Sharing generally disallows caching and delayed write-backs,
both of which are critical to making performance of remote memory 
feasible for applications, as we have seen in 
section~\ref{sec:performance}. Aguilera et al.~\cite{Aguilera2017}
mooted the idea of non-simultaneous sharing where at any time, 
memory is accessed exclusively by a single host, but across 
time it can be accessed by many; which is enough for some common 
distributed workloads like Map Reduce.


\vspace{3pt}
\noindent \uline{The extent of memory-side compute.}
Most previous systems differ significantly on the 
amount of remote/near-memory compute capabilities they expect 
of the memory server. At the bare minimum, the memory server 
should serve remote memory allocations and accesses while 
managing its local memory. Some systems go further and offload 
other runtime tasks ranging from address mapping, replication and 
persisting to disk~\cite{legoos} to garbage collection~\cite{semeru} 
on the memory servers. Others~\cite{aifm,literdma} introduce 
RPCs that applications can run on the memory server, which can 
be of arbitrary complexity. This may be a slippery slope as it adds 
complexity to the memory server that we may not want, at least in 
the hardware disaggregated setting where there is limited compute 
on memory nodes. Even in the traditional setup, we are still 
looking to decouple CPU from memory, and  increasing compute 
complexity on the remote memory side does not help with 
disaggregation and complicates resource accounting. 
Perhaps a middle ground is to find a set of hardware primitives 
(as in~\cite{Aguilera2019}) for common remote side operations and 
expose only these set as RPCs through platforms like 
Storm~\cite{storm} and Strom~\cite{strom} that enable  
implementing these RPCs on the remote NICs. 


\vspace{3pt}
\noindent \uline{Exploiting other new trends 
in the datacenters.}
With datacenters increasingly employing heterogeneous 
hardware, disaggregated memory should be made accessible 
to these devices as well. This raises the question of 
the best interface for accessing memory from such  
custom devices like GPUs or FPGAs, and how feasible they 
are for implementation on these devices. For example, 
traditional paging-based interfaces cannot be 
implemented on custom hardware that lacks the TLB and MMU 
hardware (LegoOS~\cite{legoos} moves the translation hardware 
to memory nodes making it friendlier for such hardware). 
Another related trend is the 
adoption of programmable networking hardware like 
NICs and switches. Switches have been used to 
accelerate applications through in-switch caching, load
balancing, etc. Similar acceleration opporunities may be found 
in case of diaggregated memory systems for global tasks like 
memory management and load balancing in the critical path.

% Kona provides a first view...
% Hardware like Kona is good but handling the
% complexity of replication and security in the 
% hardware might be hard?
% 3. How might the new hardware architectures e.g., 
% emerging networking interconnects like OmniPath 
% affect the design of such systems? 
% 13. Currently can't scale to more than rack-scale or a few 
% racks due to interconnect latency. but 
% as new technologies and runtimes within 
% the server cut the latencies, it will perhaps 
% allow more interconnect distance between the 
% servers, and grow to an entire datacenter. 


\vspace{3pt}
\noindent \uline{The need for a standard benchmark.}
A disaggregated system is expected to be general and support
a wide variety of applications, not just interface-wise but 
also w.r.t. the performance impact. As many recent 
systems show in their evaluation~\cite{netdisagg,legoos,fastswap}, 
performance degradation can vary dramatically across 
applications depending on the amount of local memory.
Similarly, different applications react differently to 
perfomance optimizations like caching and prefetching,
based on their memory access and locality patterns. 
Given this heterogeneity, the benchmark for evaluating a 
diaggregated memory system should include a variety of 
applications ranging from compute to memory intensive ones. 
It is also preferable to have a standard one to objectively 
compare various systems and better quantify their differences 
(all recent systems chose their own set of custom benchamrking 
applications and very few of them intersect making the 
comparison across the spectrum infeasible as the benchmark
may be biased in favor of their own systems). 


% IMP QUESTIONS FOR THE TALK
% - What's does the future look like for memory disaggregation?
%   Is it gonna play out?
%