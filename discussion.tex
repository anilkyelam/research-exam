\section{DISCUSSION}
\label{sec:discussion}
\anil{WORKING HERE}
\mc{Future challenges and opportunities in topics described 
above or any new ones that needs to be solved before 
disaggregation could become feasible/or to make it more 
adoptable. Look at \cite{Aguilera2017} and assess what 
the paper got it right and what it did not.}

1. Does memory disaggregation make sense for 
compute-intensive applications? What applications/use-cases
is it the most suitable for? What are some apps where it 
would not work? Remote memory may not be that beneficial for 
compute-intensive applications as memory accesses 
get amortized and using disk may not be that bad. 
Then why did spark fail to run on Infiniswap? 
Because working set is big and it started thrashing? 

2. What's the future of job scheduling like 
fastswap in the context of far memory? 

3. How might the new hardware architectures e.g., 
emerging networking interconnects like OmniPath 
affect the design of such systems? 

4. Future: how can remote memory be made to work 
with VMs/cloud tenants? Some works already 
consider hypervisors:~\cite{Lim2012}. 

5. Is remote memory-based app feasible performance-wise?
Yes, since most of the memory tends to be cold and 
if caching is done right, it may not affect the 
performance that much.. look at the eval sections of 
various systems to prove this?

6. Could managed/userspace runtimes be the future of 
remote memory systems? Is far memory awareness really 
important in the longer term? What kind of adoption did 
NUMA-aware data structures see? 

7. What's does the future look like for memory disaggregation?
Is it gonna play out?

8. What server capabilities are reasonable? Should function 
shipping be allowed?

9. Which system comes close? Where can it get better?
only infiniswap and legoos checks most boxes but 
it targets future hardware.
i.e., what would you add to LegoOS?
pure userspace runtimes are not it. but if co-design is 
a must, then may be kernel-based, ioctl runtimes are 
probably the way?

10. What interface works best for accelerators, keeping 
the hardware heterogeneity? 

11. What is the bare minimum piece of runtime that must run 
locally? Perhaps parts of a monolithic OS like Linux can 
still go on remote memory leaving a very thin kernel 
required for supporting caching and remote memory ops. 
What are the benefits? Is Yizhou's network disaggregation 
related?

12. Can we take lessons from somewhere regarding the 
transparency/generality vs performance debate?

13. Currently can't scale to more than rack-scale or a few 
racks due to interconnect latency. but 
as new technologies and runtimes within 
the server cut the latencies, it will perhaps 
allow more interconnect distance between the 
servers, and grow to an entire datacenter. 

14. Hardware like Kona is good but handling the
complexity of replication and security in the 
hardware might be hard?

15. Challenges that come with accelerators 

16. Opportunities with programmable networking
using it for memory and network load-balancing 
prevent congestion 

\subsection{Future}
1. Semeru + Far Memory Data structures: currently semeru 
partitions and spreads the heap over multiple machines 
with no regard of data structure optimizations. 

2. Kona/AIFM + Leap Prefetching may help with 
read I/O amplification as well.

3. Traditionally less focus on the intricacies of server-side 
memory management like avoiding fragmentation etc.

4. Security and Isolation, specifically performance isolation
does not exist. 


5. Need to extend it for virtualization solutions with 
proper SLO guarantees to serve the cloud model. 
Making it available for virtual machines through the 
hypervisor, there hasn't been any work in this direction.
Through Hypervisor, would add an additional layer 
introducing overheads and gap between apps. 


5. Wide ranging benchmarks? Spark failure shows that 
bad locality can hurt app performance beyond salvage due 
to thrashing. A standard benckmark with wide range of 
applications to properly compare across systems.